\section{Evaluation}
\label{sec:evaluation}
We evaluate Helios under microbenchmarks and Ethereum mainnet workloads to answer three questions:
\textbf{RQ1} (Optimization Overhead): What are the time and space costs of Helios's tracing and optimization?
\textbf{RQ2} (Performance Gains): How much speedup does Helios achieve over modern EVM interpreters and existing optimizations?
\textbf{RQ3} (System Applicability): How well do Replay and Online modes serve their respective deployment scenarios?

\subsection{Experimental Setup}
All experiments run on an AWS r7i.2xlarge instance with 8 vCPUs and 64~GB memory. We compare Helios against four baselines representing the state-of-the-art. Geth v1.9.9~\cite{geth} serves as the representative Go-based client, while Revm v22.0.1~\cite{revm} represents high-performance Rust interpreters. We also evaluate Forerunne~\cite{forerunner}, applying its full-context tracing to both Geth and Revm. Finally, we include Revmc v0.1.0~\cite{revm}, a JIT compiler that translates EVM bytecode to Rust. Revmc bundles its own Revm version, which may differ from our standalone Revm baseline.

For microbenchmarks we use three DeFi workloads of increasing complexity: \emph{ERC20-Transfer}, \emph{Uniswap-V2-Swap-1hop}, and \emph{Uniswap-V2-Swap-4hop}. Each transaction is executed 100 times with warm caches and we report medians. For mainnet evaluation we replay 5{,}000 consecutive Ethereum blocks (\#19{,}476{,}587–\#19{,}481{,}586), containing 921{,}786 transactions, of which 567{,}372 are contract invocations. Pure ETH transfers are excluded because they do not involve EVM execution. Throughout all experiments, Helios maintained bit-level state consistency with the Revm baseline, empirically validating the \textit{safety} of our hybrid execution model.

\subsection{Microbenchmark Performance}

\subsubsection{Optimization Overhead (RQ1)}

\input{table/optimization-overhead}

Table~\ref{tab:optimization_overhead} summarizes tracing latency and artifact size compared to Forerunner. For ERC20-Transfer, Helios records a path in 23.2~$\mu$s, yielding a 12.5$\times$ reduction over Forerunner-Geth and comparable latency to our Forerunner-Revm reimplementation. As contract complexity increases, tracing cost becomes dominated by the number of executed instructions, and Helios still achieves 1.4–2.4$\times$ lower tracing time than Forerunner-Geth and about 1.2$\times$ lower than Forerunner-Revm.

The storage savings are more pronounced. For the three benchmarks, Helios reduces artifact size by 82.7$\times$, 13.0$\times$, and 16.2$\times$ compared to Forerunner-Geth, and by 9.4–16.4$\times$ compared to Forerunner-Revm. The resulting artifacts (4.8--122.9~KB) are small enough to be cached in memory, making online management practical.

\subsubsection{Execution Speedup (RQ2)}

\input{figures/micro-bench-exec}
\input{table/geth-vs-revm}

Table~\ref{tab:substrate_comparison} compares Geth-based and Revm-based systems. Forerunner-Geth accelerates Geth Native by up to 5.8$\times$ but still remains substantially slower than unoptimized Revm on simple workloads, highlighting that optimization benefits are bounded by the efficiency of the underlying interpreter. We therefore focus on Revm-based systems when comparing optimization strategies.

Figure~\ref{fig:microbench_speedup} reports speedups over Revm Native. Helios achieves 1.14$\times$, 2.00$\times$, and 1.77$\times$ speedup on ERC20-Transfer, Uniswap-1hop, and Uniswap-4hop respectively. Gains are largest on medium and complex swaps, where repetitive arithmetic and control-flow patterns allow the SSA optimizer to eliminate redundant work.

Against Forerunner-Revm, Helios improves performance by 4–34\% across benchmarks. On Uniswap-1hop, Forerunner-Revm achieves 1.58$\times$ speedup, whereas Helios reaches 2.00$\times$, reducing execution time from 39.2~$\mu$s to 31.0~$\mu$s. The advantage comes from lightweight artifacts that are faster to load and execute.

Revmc exhibits mixed performance. While it is 11\% slower than Revm Native on ERC20-Transfer, it achieves speedups of 1.46$\times$ and 1.36$\times$ on the two Uniswap workloads, respectively. JIT compilation amortizes better on longer paths, but still underperforms Helios. These results suggest that interpreter-level SSA specialization can achieve competitive \textit{efficiency}.

\subsubsection{Opcode Reduction vs. Speedup (RQ2)}

\input{table/opcode-reduction}

Table~\ref{tab:opcode_reduction} indicates that SSA optimization eliminates 76\% to 82\% of dynamic opcodes across all benchmarks, where constant folding accounts for approximately 98\% of the reduction. However, the resulting speedups of 1.14$\times$ to 2.00$\times$ do not scale linearly with this reduction in instruction count.

\input{table/overhead-breakdown.tex}

We analyze this discrepancy in Table~\ref{tab:overhead-breakdown} by decomposing the micro-architectural latency of a hash-intensive workload. The breakdown reveals that computationally expensive operations such as \texttt{KECCAK256}~\cite{keccak} dominate execution and account for over 60\% of the wall-clock latency. Since Helios delegates these operations to the native host to ensure safety, their fixed costs limit the theoretical maximum speedup. Within the optimizable scope, Helios reduces interpretation overhead including stack manipulation and gas metering by 30.5\%, which decreases the per-iteration latency from 134ns to 93ns. Consequently, while the register-based model streamlines execution logic, the overall performance gains remain bounded by the inherent computational intensity of cryptographic.

\subsection{Mainnet Workload Analysis}
\subsubsection{Storage Overhead and Coverage (RQ1)}

We next examine the cost of storing optimization artifacts on real workloads. Caching all unique paths observed in 5{,}000 blocks yields 426~MB of artifacts, corresponding to 41.9\% overhead relative to raw block data (Figure~\ref{fig:storage_growth}). Overhead grows sub-linearly: the first 1{,}000 blocks incur 52.2\% overhead, which drops as later blocks increasingly reuse existing paths.

\input{table/storage-coverage-tradeoff}

Figure~\ref{fig:node_distribution} characterizes the complexity of cached SsaGraphs. The distribution is bimodal, with peaks at 201--500 nodes (30.81\%) and 1K--2K nodes (26.31\%). The median graph contains 494 nodes, corresponding to approximately 15.8~KB when each node stores a 256-bit value---half the 32~KB footprint of a standard 1024-slot EVM stack. This validates the register-per-node design described in \S\ref{sec:ssa_optimizer}: typical paths yield compact graphs that fit comfortably in cache. Only 0.53\% of paths exceed 10,000 nodes; these trigger the complexity threshold and fall back to native interpretation.

To exploit path locality, we apply frequency-based filtering and keep only paths that execute at least $f$ times during the warmup period (Table~\ref{tab:storage_coverage_tradeoff}). A threshold of $f\geq 10$ reduces storage to 50~MB (4.9\% overhead) while still covering 96.5\% of contract executions and achieving a 58.0\% Top-1 prediction hit rate. More aggressive thresholds further shrink storage but lose coverage, so we use $f\geq 10$ for Online deployment. The stability of storage overhead under varying thresholds also demonstrates Helios's \textit{robustness} against path explosion, as adversarially generated cold paths are naturally excluded from the cache.

\subsubsection{End-to-End Speedup (RQ2 \& RQ3)}
\input{figures/storage-growth}

Figure~\ref{fig:combined_speedup} presents the block-level speedup distribution across all three deployment configurations. We summarize the key observations below.

\paragraph{Replay Mode.}
Replay mode assumes a precomputed transaction plan and represents the best-case scenario for archive nodes. It achieves a median speedup of 6.60$\times$ over Revm Native, with the 75th and 90th percentiles reaching 13.88$\times$ and 21.43$\times$, respectively. Only 5.4\% of blocks exhibit slowdown. The distribution skews toward high speedups (10--20$\times$ and $\geq$20$\times$ bins account for 37.6\% of blocks), confirming that deterministic path reuse enables substantial acceleration for historical block re-execution.

\paragraph{Online Mode (Full Cache).}
Without frequency filtering, Online mode must predict execution paths using only the current CallSig. It reaches a median speedup of 4.56$\times$, with 75th and 90th percentiles at 7.12$\times$ and 9.85$\times$. The distribution concentrates in the 3--10$\times$ range (63.0\% of blocks), reflecting the overhead of runtime prediction and occasional mispredictions. Still, 8.8\% of blocks are slower than baseline---higher than Replay mode due to cache lookup costs and fallback penalties.

\paragraph{Online Mode (Filtered, $f \geq 10$).}
Frequency-based filtering reduces storage to 50~MB but shifts the distribution leftward. The median speedup drops to 2.05$\times$, and the 1--2$\times$ bin now dominates (36.2\% of blocks). However, the 90th percentile remains high at 9.01$\times$, indicating that hot paths still benefit substantially. This trade-off suits latency-sensitive validators who prioritize storage efficiency over median-case acceleration.

\paragraph{Cross-Mode Comparison.}
The three distributions reveal a clear hierarchy: Replay $>$ Online (full) $>$ Online (filtered). The gap between Replay and Online stems from three factors: (i) Top-1 prediction covers only 58.0\% of executions under filtering; (ii) transaction-level rollback discards partially optimized work on mispredictions; and (iii) prediction overhead becomes visible when optimized paths execute in microseconds. Nevertheless, the ability to serve both throughput-oriented archival replay and latency-sensitive real-time validation from a unified artifact set highlights the \textit{versatility} inherent in our unified architectural design. We discuss potential extensions in \S\ref{sec:discussion}.
\input{figures/combined-mainnet}  % 新的合并图表
