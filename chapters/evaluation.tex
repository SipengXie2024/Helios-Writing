\section{Evaluation}
\label{sec:evaluation}
This section evaluates Helios under both controlled microbenchmarks and real-world mainnet workloads to assess its effectiveness as a path-driven execution engine for EVM optimization. The evaluation aims to answer three core research questions:

\textbf{RQ1: Optimization Overhead.} We compare Helios's lightweight path tracing against full contract tracing in terms of time and space cost, assessing whether the overhead remains acceptable for online deployment in production blockchain nodes.

\textbf{RQ2: Performance Gains.} We evaluate the speedup achieved by Helios relative to native EVM execution and existing optimization approaches, examining whether the path-driven optimization strategy delivers consistent improvements across diverse smart contract workloads.

\textbf{RQ3: System Applicability.} We assess the performance of Replay mode and Online mode in their respective target scenarios---archive node acceleration and full node optimization.

The evaluation proceeds in three stages. \S\ref{sec:experiment-setup} describes the experimental setup, including hardware configuration, baseline systems, and workload selection. \S\ref{sec:micro-benchmark} presents microbenchmark results for three representative DeFi transactions, isolating the impact of path tracing overhead, execution speedup, and SSA optimization effectiveness. \S\ref{sec:mainet} analyzes Helios's performance on mainnet blocks, validating path locality assumptions and measuring aggregate throughput improvements.

\subsection{Experimental Setup}
\label{sec:experiment-setup}
All experiments were conducted on an AWS r7i.2xlarge instance with 8 vCPUs and 64 GB of memory. Helios is implemented in Rust and compiled with release optimizations enabled. We evaluate against four baseline systems. Geth v1.9.9 serves as the reference Go-based Ethereum client for native EVM execution. Revm v22.0.1 provides a highly optimized Rust-based EVM implementation with substantially faster native performance. Forerunner is tested in two variants: the original Geth-based implementation and a reimplemented Revm version that replicates its tracing and optimization strategy. Revmc v0.1.0 represents a JIT compilation approach to EVM optimization, evaluated in both native and optimized execution modes. Revmc bundles its own Revm execution environment, which may differ from our standalone Revm v22.0.1 baseline.

\subsection{Microbenchmark Performance}
\label{sec:micro-benchmark}
We evaluate Helios using three representative DeFi transactions of increasing complexity. \textbf{ERC20-Transfer} executes 492 opcodes for token transfers, representing the most frequent transaction type on Ethereum. \textbf{Uniswap-V2-Swap-1hop} performs single-pool exchanges with 5,667 opcodes, typical of high-liquidity pairs. \textbf{Uniswap-V2-Swap-4hop} executes multi-pool routing across 18,063 opcodes, representing complex swap paths for long-tail assets. These benchmarks enable systematic analysis of optimization overhead, execution speedup relative to baselines, and the relationship between opcode reduction and performance gains. Each transaction executes 100 times with warm caches, reporting median values.

\subsubsection{Optimization Overhead}

\input{table/optimization-overhead}

This subsection addresses \textbf{RQ1} by quantifying the optimization overhead in terms of tracing time and artifact storage footprint. Table~\ref{tab:optimization_overhead} compares Helios against Forerunner's full-tracing approach on both Geth and Revm. We note that our Forerunner-Revm implementation employs intrusive instrumentation for tracing, while Helios uses a hook-based mechanism that preserves native execution performance.

Helios achieves lower tracing overhead than full-tracing approaches. For ERC20-Transfer, Helios completes path tracing in 23.2~$\mu$s, a 12.5× speedup over Forerunner-Geth. The tracing latency remains comparable to the optimized Forerunner-Revm implementation. As contract complexity increases, the advantage over Forerunner-Geth diminishes to 2.4× for Uniswap-Swap-1hop and 1.4× for Uniswap-Swap-4hop. For complex contracts, the volume of executed instructions dominates tracing cost regardless of strategy.

Storage efficiency gains are more pronounced. Helios achieves 82.7× compression over Forerunner-Geth for ERC20-Transfer, 13.0× for Uniswap-Swap-1hop, and 16.2× for Uniswap-Swap-4hop. Relative to Forerunner-Revm, Helios achieves 9.4× compression for ERC20-Transfer, 9.2× for Uniswap-Swap-1hop, and 16.4× for Uniswap-Swap-4hop. The absolute artifact sizes of 4.8~KB, 70.2~KB, and 122.9~KB enable efficient in-memory caching without memory pressure. For the most complex benchmark, Helios stores the optimized path in 122.9~KB compared to Forerunner-Revm's 2,017.5~KB, a 16.4× reduction that allows hundreds of cached paths to fit within typical L3 cache budgets. This sub-megabyte footprint per contract makes Helios practical for online deployment where storage overhead directly impacts scalability.

With tracing latency in the sub-millisecond range and storage requirements measured in kilobytes, we now evaluate the execution performance gains.

\subsubsection{Execution Performance}

\input{figures/micro-bench-exec}
\input{table/geth-vs-revm}

This subsection addresses \textbf{RQ2} by evaluating execution speedup on optimized paths. Figure~\ref{fig:microbench_speedup} presents acceleration factors for Revm-based systems normalized against Revm Native, which represents state-of-the-art substrate efficiency. Table~\ref{tab:substrate_comparison} compares Geth-based and Revm-based implementations.

Helios achieves speedups of 1.14×, 2.00×, and 1.77× for ERC20-Transfer, Uniswap-Swap-1hop, and Uniswap-Swap-4hop. Performance peaks at medium-complexity Uniswap-1hop, where repetitive automated market maker computations create optimization opportunities. Helios reduces execution time from 62.0~$\mu$s to 31.0~$\mu$s by eliminating redundant arithmetic and logic operations through SSA-based path specialization. The modest gain on simple ERC20-Transfer suggests highly optimized baselines leave limited room for interpreter-level optimization.

\textbf{Substrate Efficiency Constraint.}
Substrate selection fundamentally constrains optimization effectiveness. While Forerunner-Geth reduces Geth Native execution time by 2.5× to 5.8× across benchmarks, optimized Geth execution at 35.75~$\mu$s for ERC20-Transfer remains 7.2× slower than unoptimized Revm at 4.94~$\mu$s. This disparity diminishes on complex workloads, with Forerunner-Geth achieving near-parity on Uniswap-4hop at 167.38~$\mu$s versus 172.49~$\mu$s. Even advanced optimization cannot overcome substrate efficiency gaps on simple workloads. We therefore focus on Revm-based systems to isolate optimization technique effectiveness.

\textbf{Lightweight vs. Full-Context Tracing.}
Comparing Revm-based systems isolates tracing strategy impact from implementation artifacts. Both Helios and Forerunner-Revm capture optimization opportunities from executed traces at path-level granularity. The key distinction is tracing scope: Helios employs lightweight stack-only tracing, whereas Forerunner performs full-context tracing capturing stack frames, memory snapshots, and contract state. Helios outperforms Forerunner-Revm by 1.04×, 1.27×, and 1.34× across benchmarks. On Uniswap-1hop, Helios achieves 2.00× versus Forerunner-Revm's 1.58×, reducing execution time from 39.20~$\mu$s to 30.96~$\mu$s.

This advantage stems from compact optimization artifacts. Lightweight tracing generates 4.87~KB versus 402.8~KB for full-context tracing. Reduced artifact size enables faster loading and deserialization when replaying optimized paths, while simpler trace structure reduces interpreter overhead by processing fewer metadata fields per instruction. This decreases cache pressure and per-instruction cost. The lightweight approach sacrifices no optimization effectiveness for deterministic execution paths, which constitute the majority of mainnet transactions.

\textbf{JIT Compilation Tradeoffs.}
Revmc's just-in-time compilation demonstrates mixed effectiveness. For ERC20-Transfer, Revmc underperforms the native baseline by 11\% despite using pre-compiled machine code. Several factors likely contribute. Modern interpreters like Revm employ micro-optimizations such as computed gotos, inline caching, and specialized fast paths. These optimizations benefit from whole-program compilation. JIT-generated code, constrained by runtime generation, may fail to achieve comparable efficiency. This limitation becomes particularly significant for short execution sequences where instruction cache effects and setup overhead dominate.

Performance improves on complex workloads to 1.46× and 1.36× for Uniswap-1hop and Uniswap-4hop, suggesting longer execution sequences better amortize fixed overheads. However, Revmc remains inferior to Helios at 2.00× and 1.77× on these benchmarks, with geometric mean speedup of 1.24× versus 1.64×.

These results suggest interpreter-level optimization offers advantages over JIT compilation for EVM workloads. Helios performs SSA transformations on abstract opcode sequences and replays optimized paths through Helios Engine, preserving micro-architectural optimizations in the hand-tuned interpreter while reducing instruction count. JIT compilation replaces the interpreter entirely, requiring runtime code generation to match pre-compiled interpreter efficiency. This appears challenging for microsecond-scale EVM transactions. However, definitive conclusions would require controlled experiments isolating individual optimization components.

To understand the mechanisms underlying these performance gains, we now decompose the impact of individual SSA optimization passes and examine why significant opcode reduction translates to moderate execution speedup.

\subsubsection{Opcode Reduction and Speedup Discrepancy}

This subsection provides deeper insight into \textbf{RQ2} by examining the relationship between opcode reduction and execution speedup across the three benchmarks.

\input{table/opcode-reduction}

\textbf{Optimization Effectiveness.} Constant folding dominates opcode reduction, accounting for 97.5\%, 97.8\%, and 97.9\% of eliminated instructions as is shown in Table~\ref{tab:opcode_reduction}. SSA-based dataflow analysis propagates compile-time constants through execution paths, enabling elimination of computations with known results, unreachable branches, and redundant operations. Common subexpression elimination and dead code elimination contribute 2.1-2.5\% combined, indicating limited redundancy after constant propagation. Consistent reduction rates of 76.6-82.3\% across varying workload complexities suggest SSA optimization effectiveness is largely invariant to contract scale.

\textbf{Understanding the Speedup Discrepancy.} Helios reduces opcode count by 76-82\% through SSA-based optimization, yet achieves only 1.14-2.00× speedup rather than the 4.28-5.66× theoretical prediction based on instruction count proportionality. This discrepancy arises from deliberate design constraints that prioritize economic compatibility over maximal performance gains.

Our optimization strategy targets lightweight operations such as stack manipulation, arithmetic, and control flow. These operations execute in nanoseconds individually but account for the majority of eliminated instructions. Constant folding removes these operations systematically, yielding cumulative microsecond-scale savings per transaction that aggregate to millisecond-scale reductions in block processing time across hundreds of transactions per block. Computationally intensive operations such as KECCAK256 and storage accesses remain unoptimized. As established in \S\ref{sec:motivation}, these operations involve dynamic gas metering. We leave these operations unoptimized to avoid gas metering complications.

The SSA graph execution model incurs additional overhead. Each optimized path traversal requires metadata access, register lookups, and dispatch logic. When eliminated opcodes execute in nanoseconds, these graph traversal costs become proportionally significant relative to the savings achieved. The SSA representation trades interpretation overhead for portability and analyzability without requiring JIT compilation infrastructure. The observed 1.14-2.00× speedups demonstrate that substantial opcode reduction yields measurable performance gains within these architectural constraints. \S\ref{sec:discussion} explores further optimization opportunities under the current design framework.

\subsection{Mainnet Workload Analysis}
\label{sec:mainet}

Having validated Helios's optimization effectiveness on isolated transactions, we now evaluate performance and storage overhead on real-world workloads using 5,000 consecutive mainnet blocks numbered 19,476,587 through 19,481,586. These blocks contain 921,786 total transactions, of which 567,372 are smart contract invocations. We exclude 351,212 pure transfers because they involve no contract execution and fall outside Helios's optimization scope.

We define \textit{execution coverage} as the fraction of contract executions whose PathDigest and DataKey pair matches a cached optimization artifact. Coverage quantifies cache effectiveness by measuring how many executions reuse pre-computed optimizations without re-tracing. This section quantifies storage requirements, validates speedup under production workloads, and analyzes deployment tradeoffs between Replay and Online modes.

\subsubsection{Storage Overhead}

This subsection continues addressing \textbf{RQ1} by analyzing storage requirements to assess Helios's practicality for production deployment. We generate optimization artifacts for 5,000 consecutive blocks to ensure cache warmup and path convergence, then evaluate storage-coverage tradeoffs under different caching strategies.

\input{figures/storage-growth}

\textbf{Baseline Storage Requirements.}
Figure~\ref{fig:storage_growth} shows storage growth for block data and Helios optimization artifacts across 1,000 to 5,000 blocks. Processing 5,000 blocks generates 426~MB of optimization artifacts when caching all unique execution paths without filtering, representing 41.9\% overhead relative to raw block data. Storage overhead exhibits sub-linear growth. The first 1,000 blocks incur 52.2\% overhead, decreasing to 41.9\% at 5,000 blocks due to path convergence.

\input{table/storage-coverage-tradeoff}

\textbf{Frequency-Based Filtering.}
Path locality established in \S\ref{sec:frame-level-caching} demonstrates that execution frequency follows a Pareto distribution. We exploit this property through frequency-based filtering, retaining only paths executed at least $f$ times across the 5,000-block warmup period. This approach offers implementation simplicity compared to percentile-based thresholds while naturally adapting to workload characteristics. Table~\ref{tab:storage_coverage_tradeoff} quantifies the storage-coverage tradeoff across different frequency thresholds.

Applying frequency $\geq$10 filtering reduces storage to 50~MB while maintaining 96.5\% execution coverage and 58.0\% Top-1 hit rate. More aggressive thresholds yield diminishing returns. Frequency $\geq$100 achieves only 1.9\% overhead but sacrifices 11\% execution coverage. Frequency $\geq$500 becomes impractical at 69.9\% coverage despite minimal 0.3\% storage cost. The frequency $\geq$10 threshold balances storage efficiency and coverage preservation, motivating its selection for Online mode deployment. We investigate the performance implications of this filtering strategy below.

\input{figures/combined-mainnet}

\subsubsection{Replay Mode Performance}

This subsection addresses \textbf{RQ2} and \textbf{RQ3} by evaluating Replay mode on 5,000 blocks to establish performance upper bounds under perfect path knowledge for archive node scenarios. This represents the oracle baseline where all execution paths are known deterministically.

Figure~\ref{fig:replay_speedup_dist} presents the speedup distribution. Helios achieves a median speedup of 6.60×. The distribution exhibits strong tail performance with the 75th percentile at 13.88× and the 90th percentile at 21.43×. Only 5.4\% of blocks experience slowdown relative to baseline execution. The distribution concentrates in the 5-10× range at 22.6\% and 10-20× range at 25.6\%, demonstrating consistent acceleration across diverse workloads.

Replay mode proves particularly valuable for archive nodes performing historical synchronization, where blockchain history is optimized once and replayed efficiently for subsequent queries or re-synchronizations.

\subsubsection{Online Mode Effectiveness}

This subsection addresses \textbf{RQ2} and \textbf{RQ3} by evaluating Online mode for live transaction execution where execution paths are unknown a priori. We evaluate performance on 1,000 blocks immediately following the 5,000-block warmup period, ensuring the test set contains fresh transactions not seen during cache construction. We evaluate performance under two configurations. Online mode without filtering caches all paths from the warmup period. Online mode with frequency $\geq$10 filtering retains only paths exceeding the frequency threshold, as motivated by Table~\ref{tab:storage_coverage_tradeoff}.

\textbf{Online Mode Without Filtering.}
Figure~\ref{fig:online_no_filter} presents the speedup distribution when caching all paths. Online mode achieves a median speedup of 4.56×, underperforming Replay mode's 6.60× by 31\%. The distribution concentrates in the 5-10× range at 34.0\%, with the 75th percentile at 7.12× and the 90th percentile at 9.85×. Only 8.8\% of blocks exhibit slowdown relative to baseline execution.

\textbf{Online Mode With Frequency Filtering.}
Figure~\ref{fig:online_filtered} presents the speedup distribution under frequency $\geq$10 filtering. Median speedup drops to 2.05×, representing a 55\% reduction relative to unfiltered Online mode. The distribution shifts toward lower speedup ranges, with 36.2\% of blocks concentrated in the 1-2× range. However, tail performance remains competitive at the 90th percentile with 9.01× speedup. Performance degradation primarily affects median and lower percentiles rather than tail workloads.

\textbf{Performance Degradation Analysis.}
Three factors contribute to Online mode's performance degradation relative to Replay mode. First, greedy Top-1 path selection achieves only 58.0\% hit rate under frequency $\geq$10 filtering, causing 42\% of transactions to fall back to native execution. Contracts with multiple high-frequency paths driven by input-dependent control flow cannot be fully covered by single-path caching. Second, Helios employs transaction-level rollback for path mispredictions. When a path divergence occurs during execution, the entire transaction rolls back to native execution rather than switching to an alternative path mid-execution. This all-or-nothing strategy amplifies the penalty for cache misses, as partially completed optimized execution provides no benefit. Third, per-transaction overhead from path prediction and cache lookup becomes proportionally significant when optimized paths execute in microseconds. These limitations suggest opportunities for multi-path caching, incremental rollback mechanisms, and adaptive path selection strategies, which we discuss in \S\ref{sec:discussion}.
