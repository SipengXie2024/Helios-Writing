\section{Introduction}

Public blockchains supporting smart contracts, such as Ethereum and its Layer-2 rollups~\cite{ethereum,base,arbitrum,polygon,bsc}, fundamentally operate as replicated state machines. From a data management perspective, each node functions as a deterministic transaction processor handling two distinct workloads. These workloads comprise the real-time processing of incoming transactions and the re-execution of historical transactions to verify global state transitions. As throughput and contract complexity increase, execution becomes a primary scalability bottleneck for both scenarios.

Recent acceleration strategies face distinct limitations on modern execution engines~\cite{revm,evmone}. Just-In-Time (JIT) compilation introduces security risks, such as JIT bombs, and causes gas accounting discrepancies~\cite{revmc,monad,evmjit,bnbjit,JITBomb}. Transaction-level speculative execution degrades performance on fast engines due to the \textit{Performance Paradox}~\cite{forerunner,seer}. In this phenomenon, the overhead of instrumentation, tracing, and artifact management frequently exceeds the latency of transaction execution itself. Similarly, operation-level concurrent execution incurs tracing or synchronization costs that frequently outweigh parallel gains~\cite{parallelEvm,evmTracer}.

Overcoming the performance paradox on hyper-optimized EVM clients necessitates addressing three fundamental conflicts. \textbf{First}, the tension between instrumentation overhead and execution latency. On sub-microsecond engines such as Revm~\cite{revm}, the cost of synchronous tracing often exceeds the execution time itself. Capturing sufficient data dependencies without introducing latency that negates the speedup is non-trivial. \textbf{Second}, the trade-off between artifact granularity and reuse capability. Existing transaction-level approaches suffer from a combinatorial explosion of unique execution paths, rendering cached artifacts ephemeral. The challenge lies in identifying a stable granularity that exhibits high locality across diverse transactions while maintaining manageable storage costs. \textbf{Third}, the conflict between optimization aggressiveness and strict semantic preservation. Standard compiler transformations such as instruction reordering inherently alter observable execution steps and violate the EVM's gas accounting rules. Achieving significant opcode reduction without complex runtime compensation logic remains a critical hurdle.

To assess whether these challenges can be surmounted, we analyzed Ethereum mainnet workloads and derived three critical insights. \textbf{First}, on hyper-optimized clients, native memory and storage management is already highly efficient; capturing full execution state incurs prohibitive I/O overhead while offering negligible optimization utility. Critical dependencies for valid optimization are therefore sparse and confined to stack operations, making lightweight stack-only tracing the optimal design point. \textbf{Second}, we observe pronounced frame-level path locality. The top 1\% of unique paths accounts for over 70\% of total execution time, indicating that shifting granularity from transactions to frames unlocks high cache hit rates across diverse workloads. \textbf{Third}, optimizing dynamic state-access operations inherently alters the observable gas schedule, risking consensus divergence. However, such operations are relatively sparse along hot paths, while fixed-cost computational instructions dominate. A path-driven accelerator can therefore safely bypass dynamic operations and delegate them to the native handler, focusing acceleration on the computation-heavy majority to ensure correctness by construction.

Guided by these observations, Helios introduces a new design point centered on three architectural principles. \textbf{First}, to break the performance paradox, we propose lightweight asynchronous tracing. By offloading stack-only analysis to background threads, Helios decouples trace generation from the critical path, ensuring that optimization overhead never blocks live transaction processing. \textbf{Second}, to maximize reuse, we exploit frame-level persistent caching. Unlike transaction-level approaches, Helios organizes optimization artifacts at the granularity of individual call frames, allowing diverse transactions to composably reuse cached paths. This transforms the combinatorial complexity of transaction traces into a manageable set of reusable frame graphs. \textbf{Third}, to guarantee safety, we adopt a hybrid execution model with correctness by construction. By restricting optimization to static-cost instructions and delegating dynamic-cost operations to the native engine, Helios preserves exact gas semantics inherently, eliminating the risk of economic discrepancies.

In summary, this paper makes the following contributions:
\begin{itemize}[leftmargin=0pt, itemindent=2em, labelsep=0.5em]

\item We investigate the limitations of transaction-level speculation and operation-level concurrency on modern execution engines to formulate the \textit{Performance Paradox}, where auxiliary overheads negate optimization gains. We further identify frame-level path locality and the separation of static and dynamic costs as key levers to resolve this paradox.

\item We propose a novel architecture that decouples optimization from critical execution paths via asynchronous lightweight tracing and maximizes artifact reuse through frame-level caching. This approach ensures minimal instrumentation overhead and high cache hit rates.

\item We develop the high-performance Helios Engine utilizing a register-based interpreter and bulk gas deduction to minimize execution overhead. It ensures intrinsic gas consistency through a hybrid model that strictly separates static optimizations from dynamic costs. This engine serves as the versatile runtime powering both deterministic replay and speculative execution within our unified framework.

\item We implement a prototype of Helios on Revm and evaluate it using Ethereum mainnet workloads. The results demonstrate a median speedup of 6.60$\times$ over the native baseline on historical workloads while providing effective acceleration for incoming transactions.

\end{itemize}