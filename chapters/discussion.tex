\section{Discussion}
\label{sec:discussion}

This section interprets the performance results, analyzes the limitations of the current design, and outlines future research directions.

\subsection{Interpreting the Speedup}

Helios achieves a median speedup of 6.60$\times$ in Replay Mode. This performance improvement results from the execution of optimized SSA graphs, which eliminates redundant stack operations and simplifies gas accounting for static instructions.

However, the evaluation reveals a disparity between the opcode reduction rate and the actual execution speedup. While SSA optimization removes approximately 80\% of instructions, the microbenchmark speedups range from 1.14$\times$ to 2.00$\times$. This discrepancy indicates that the overhead of the Traced Interpreter currently constrains the potential performance gains. The management of execution metadata and the interpretation of the graph structure introduce costs that are comparable to the savings from instruction elimination. Furthermore, unoptimized heavy instructions continue to dominate the execution time in certain workloads.

A promising avenue for future work is implementing JIT or Ahead-Of-Time (AOT) compilation to eliminate interpretation overhead and fully leverage the instruction reduction achieved by our SSA optimizer. The register-based design of the SsaGraph maps naturally to modern CPU architectures, facilitating this transition. Furthermore, unlike traditional bytecode JITs that face "JIT bomb" risks, the acyclic and strictly bounded nature of the SsaGraph offers an opportunity for safe compilation, avoiding complexity explosion attacks.

\subsection{Online Mode Challenges}

Online Mode performance decreases when frequency-based filtering is applied. Our coverage table analysis reveals a gap between the Top-1 prediction coverage and actual execution coverage. This suggests that the current simplicity-first strategy does not fully exploit the optimized paths.

We explored alternative strategies to address this limitation, including a race-parallel execution model. In this approach, the system caches the Top-K paths and executes them concurrently, committing the result of the first successful path or falling back to native execution if all fail. However, microbenchmarks showed that the overhead of the parallel framework outweighed the benefits in our initial exploration. Native execution completed in 60 $\mu$s, whereas the race-parallel implementation increased latency to 70 $\mu$s, compared to 30 $\mu$s for the baseline optimized execution in the Uniswap-1hop case. Efficiently leveraging multi-path caching remains an open problem.

Another factor is the transaction-scoped fallback mechanism. Currently, a single frame prediction failure triggers a rollback of the entire transaction to native EVM, underutilizing successful frame-level predictions. A potential solution is frame-level fallback, where only the failed frame reverts to native execution while subsequent frames continue using cached paths. However, this introduces a new attack vector: adversaries could craft malicious transactions that deliberately trigger frequent engine switching to degrade performance---we term this ``de-optimization bombs.'' To defend against such attacks, a circuit breaker could limit the number of fallbacks per transaction. The system could also explore chained predictions, where one optimized frame's output directly triggers speculative execution of the next. These strategies present trade-offs between simplicity and coverage that merit further study.

\subsection{System Scalability}

Beyond execution logic, system scalability presents further optimization opportunities. While our initial exploration of instruction-level parallelism (ILP)~\cite{ILP} showed limited improvement due to synchronization overhead, parallel execution potential remains. The current dependency graph modeling could benefit from advanced ILP techniques in compiler theory. Future research could investigate sophisticated scheduling algorithms or hardware-assisted primitives to unlock the parallelism inherent in SsaGraph.

Finally, the current Path Cache implementation prioritizes low storage overhead with a simple persistence and pruning policy. As the system scales to handle larger state histories, more mature caching strategies may be required. Future work could explore tiered storage architectures that balance hit rate, retrieval latency, and storage cost, potentially leveraging external high-performance key-value stores for long-term artifact persistence.