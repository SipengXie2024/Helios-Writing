\section{Discussion}
\label{sec:discussion}

This section interprets the performance results, analyzes the limitations of the current design, and outlines future research directions.

\subsection{Interpreting the Speedup}

Helios achieves a median speedup of 6.60$\times$ in Replay Mode. This performance improvement results from the execution of optimized SSA graphs, which eliminates redundant stack operations and simplifies gas accounting for static instructions.

However, the evaluation reveals a disparity between the opcode reduction rate and the actual execution speedup. While SSA optimization removes approximately 80\% of instructions, the microbenchmark speedups range from 1.14$\times$ to 2.00$\times$. This discrepancy indicates that the overhead of the Traced Interpreter currently constrains the potential performance gains. The management of execution metadata and the interpretation of the graph structure introduce costs that are comparable to the savings from instruction elimination. Furthermore, unoptimized heavy instructions continue to dominate the execution time in certain workloads.

A promising avenue for future work is implementing JIT or Ahead-Of-Time (AOT) compilation to eliminate interpretation overhead and fully leverage the instruction reduction achieved by our SSA optimizer. The register-based design of the SsaGraph maps naturally to modern CPU architectures, facilitating this transition. Furthermore, unlike traditional bytecode JITs that face "JIT bomb" risks, the acyclic and strictly bounded nature of the SsaGraph offers an opportunity for safe compilation, avoiding complexity explosion attacks.

\subsection{Opportunities for Further Optimization}

Online Mode achieves substantial speedups in common cases, and our analysis reveals additional optimization opportunities when frequency-based filtering is applied. The gap between Top-1 prediction coverage and actual execution coverage suggests that richer path-selection strategies could further improve performance.

We have begun exploring such strategies, including a race-parallel execution model that caches the Top-K paths and executes them concurrently. The system commits the result of the first successful path or falls back to native execution if all paths fail. Initial microbenchmarks show that coordination overhead currently limits the benefits of this approach, but lightweight synchronization primitives and speculative result caching offer promising directions to reduce this overhead.

A second opportunity lies in refining fallback granularity. The current transaction-scoped fallback reverts the entire transaction to native EVM upon a single frame prediction failure. A finer-grained, frame-level fallback would revert only the failed frame while continuing to use cached paths for subsequent frames, thereby increasing effective coverage. This design requires safeguards against adversarial inputs that deliberately induce frequent engine switching. Potential defenses include per-transaction fallback limits and chained predictions, where one optimized frame's output directly triggers speculative execution of the next. We plan to investigate these directions in future work.

\subsection{System Scalability}

Beyond execution logic, system scalability presents further optimization opportunities. While our initial exploration of instruction-level parallelism (ILP)~\cite{ILP} showed limited improvement due to synchronization overhead, parallel execution potential remains. The current dependency graph modeling could benefit from advanced ILP techniques in compiler theory. Future research could investigate sophisticated scheduling algorithms or hardware-assisted primitives to unlock the parallelism inherent in SsaGraph.

Finally, the current Path Cache implementation prioritizes low storage overhead with a simple persistence and pruning policy. As the system scales to handle larger state histories, more mature caching strategies may be required. Future work could explore tiered storage architectures that balance hit rate, retrieval latency, and storage cost, potentially leveraging external high-performance key-value stores for long-term artifact persistence.