\section{Discussion}
\label{sec:discussion}

This section interprets the performance results, analyzes the limitations of the current design, and outlines future research directions.

\subsection{Interpreting the Speedup}
\label{sec:interpret-speedup}

Prior work on speculative EVM execution has primarily focused on path identification as the central challenge, implicitly treating execution acceleration as a straightforward byproduct once the correct path is determined. Seer~\cite{seer}, for instance, concentrates on branch prediction without detailing how the predicted path translates into wall-clock savings. Forerunner~\cite{forerunner} proposes a simplified S-EVM but does not elaborate on managing micro-architectural costs during interpretation.

Our experience suggests that achieving speedup on a highly optimized baseline such as Revm is non-trivial. Although our SSA optimization removes approximately 80\% of instructions, the microbenchmark speedups range only from 1.14$\times$ to 2.00$\times$. This gap indicates that the overhead of interpreting an intermediate representation can readily offset the gains. The memory access patterns for graph nodes and the management of virtual registers are particularly costly. Furthermore, unoptimized heavy instructions continue to dominate execution time in certain workloads.

Helios overcomes this overhead through four key design decisions that collectively yield a median speedup of 6.60$\times$ in Replay Mode. First, by tracking dependencies only for stack data, we constrain node size and minimize memory access latency. Second, frame-level granularity naturally bounds the node count and register demand per execution unit, substantially reducing memory allocation overhead compared to transaction-level tracing. Third, our SSA optimizer aggressively applies constant folding and dead code elimination, alleviating the interpreter's dispatch burden. Fourth, blocked gas metering aggregates cost deductions, further reducing the frequency of accounting operations.

We posit that surpassing the current performance ceiling may require transitioning from interpretation to JIT or AOT compilation, eliminating the virtual register abstraction entirely. The register-based design of the SsaGraph maps naturally to modern CPU architectures, facilitating this transition. Unlike traditional bytecode JITs that face ``JIT bomb'' risks, the acyclic and strictly bounded nature of the SsaGraph enables safe compilation without complexity explosion attacks.

\input{figures/node-distribution}

\subsection{Opportunities for Further Optimization}

Our evaluation identifies three directions for further improvement: path selection, fallback granularity, and cache granularity.

Online Mode achieves substantial speedups in common cases, yet the gap between Top-1 prediction coverage and actual execution coverage suggests room for richer path-selection strategies. We have begun exploring a race-parallel model that caches the Top-K paths and executes them concurrently, committing the first successful result or falling back to native execution if all paths fail. Initial microbenchmarks show that coordination overhead currently limits the benefits of this approach. Lightweight synchronization primitives and speculative result caching offer promising directions to reduce this overhead.

A second opportunity lies in fallback granularity. The current transaction-scoped fallback reverts the entire transaction to native EVM upon a single frame prediction failure. A finer-grained, frame-level fallback would revert only the failed frame while continuing to use cached paths for subsequent frames, thereby increasing effective coverage. This design requires safeguards against adversarial inputs that deliberately induce frequent engine switching. Potential defenses include per-transaction fallback limits and chained predictions, where one optimized frame's output directly triggers speculative execution of the next.

A third direction involves reducing cache granularity from call frames to basic blocks. Basic-block granularity offers superior composability in principle, allowing the engine to stitch cached blocks into execution paths never observed in their entirety. However, this approach introduces a trade-off between composability and overhead. As discussed in \S\ref{sec:interpret-speedup}, achieving speedup on an optimized engine is non-trivial. The cost of managing checkpoints and context switches at every basic-block boundary may exceed the savings from the optimized blocks themselves. Moreover, reconstructing consistent execution state from fine-grained checkpoints requires rigorous validation to preserve semantic correctness. Future research should investigate whether basic-block composability outweighs the administrative cost of frequent checkpointing, or whether a hybrid strategy that dynamically coalesces basic blocks into larger superblocks can balance this trade-off.

\subsection{System Scalability}

Beyond execution logic, system scalability presents further optimization opportunities. While our initial exploration of instruction-level parallelism (ILP)~\cite{ILP} showed limited improvement due to synchronization overhead, parallel execution potential remains. The current dependency graph modeling could benefit from advanced ILP techniques in compiler theory. Future research could investigate sophisticated scheduling algorithms or hardware-assisted primitives to unlock the parallelism inherent in SsaGraph.

Finally, the current Path Cache implementation prioritizes low storage overhead with a simple persistence and pruning policy. As the system scales to handle larger state histories, more mature caching strategies may be required. Future work could explore tiered storage architectures that balance hit rate, retrieval latency, and storage cost, potentially leveraging external high-performance key-value stores for long-term artifact persistence.