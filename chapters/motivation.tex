\section{Motivation}

Traditional path-driven optimization systems like Forerunner and Seer achieve 5–8× speedups on classical EVM interpreters such as Geth. However, when applied to modern, highly-optimized implementations like Revm, these gains diminish sharply. For identical bytecode, speedups drop from 5.0× on Geth to only 1.5× on Revm. This degradation exposes fundamental limitations in existing optimization strategies when baseline execution is already fast. Through systematic analysis across four critical design dimensions of optimization paradigm, tracing strategy, artifact organization, and correctness guarantees, we derive a set of principled design choices that enable effective optimization on modern EVMs. Each choice addresses specific bottlenecks while establishing constraints that guide subsequent decisions, ultimately converging on Helios's architecture.

\input{figures/loop-hoisting-gas}

\subsection{Path-Driven over Contract-Driven Optimization}
\label{sec:path-vs-contract}
EVM optimization can follow two competing paradigms. Contract-driven optimization, exemplified by JIT compilation approaches, optimizes entire contracts. Path-driven optimization, employed by systems like Forerunner and Seer, optimizes only executed traces. This foundational choice establishes the scope and safety model for all subsequent design decisions.

Contract-driven approaches compile entire contract bytecode into optimized native code, enabling aggressive global optimizations such as loop hoisting, dead code elimination, and cross-basic-block register allocation. These transformations can yield substantial performance improvements on hot contracts. However, this paradigm faces two critical challenges in production blockchain environments.

First, unrestricted JIT compilation introduces severe security risks through JIT Bomb attacks. Malicious bytecode can exploit compilation complexity by using deeply nested control flow or pathological loop structures to force the JIT compiler into exponential-time compilation or excessive memory consumption. This denial-of-service vector is particularly dangerous in blockchain contexts where attackers can trivially deploy adversarial contracts and force validators to compile them. Existing JIT-based EVM implementations address this threat through contract whitelisting, restricting optimization to pre-approved contracts. This mitigation severely limits applicability, as the majority of contract invocations fall outside whitelists, rendering the optimization ineffective for general workloads.

Second, aggressive global optimizations frequently violate gas-semantic equivalence. Figure~\ref{fig:loop-hoisting-gas} illustrates this problem through a representative example where loop hoisting reduces the number of storage reads, thereby changing the gas consumption profile. While this transformation improves performance, it creates economic misalignment between charged fees and actual resource usage. This divergence presents validators with an undesirable choice between overcompensating for work performed or undercompensating relative to network standards, potentially discouraging adoption of the optimization.

Path-driven optimization offers a different trade-off. By optimizing only actually-executed paths rather than entire contracts, this paradigm inherently bounds optimization costs per path, eliminating JIT Bomb vulnerabilities. Every path processed represents real execution that already occurred, ensuring compilation work scales linearly with actual usage rather than potential attack surface. This enables universal deployment without whitelists. Any contract invoking frequently-executed paths automatically benefits from optimization, regardless of bytecode complexity or origin.

However, path-driven systems must still address gas-semantic preservation explicitly. While this paradigm avoids the cross-contract global optimizations that complicate gas preservation in JIT approaches, it does not automatically guarantee gas equivalence. Existing path-driven systems such as Forerunner and Seer do not explicitly discuss this equivalence in their design. Helios achieves precise gas-semantic preservation through specific design choices, particularly the combination of stack-only tracing, frame-level organization, and path-local transformations, as we will demonstrate in \S\ref{sec:gas-semantic-equivalence}.

The choice of path-driven optimization establishes our first architectural constraint. We optimize execution traces post-facto rather than contracts a priori. This decision prioritizes safety and universal applicability over maximal theoretical performance, setting the foundation for subsequent design choices.

\input{table/motivation-overhead}

\subsection{The Overhead of Existing Path-Driven Approaches}
Having established the path-driven paradigm, we now examine why existing implementations struggle on modern EVMs. The core issue is a performance paradox. Detailed tracing, once a minor cost on slow interpreters, becomes the primary bottleneck when baseline execution is already fast.

This paradox manifests most severely in the memory I/O cost of optimization artifacts. Table~\ref{tab:motivation-overhead} demonstrates this performance degradation through measurements on a representative Uniswap V2 single-hop swap. Forerunner achieves substantial speedups on Geth but experiences diminished returns on Revm despite generating identical 663 KB trace artifacts. We note that since Forerunner does not provide a native Revm implementation, our measurements employ a functionally equivalent mock implementation that replicates Forerunner's tracing and optimization strategy on the Revm runtime. The reduced baseline execution time on modern EVMs exposes artifact overhead as the dominant bottleneck. When baseline execution completes in tens of microseconds, loading and parsing trace artifacts of this magnitude consumes time comparable to the computational work being optimized.

However, artifact I/O represents only part of the overhead. Full-tracing approaches that maintain custom execution context management—including call frames, memory snapshots, and storage deltas—forgo the host EVM's optimized native implementations. For complex transactions with artifacts exceeding hundreds of kilobytes, I/O latency dominates. For simple transactions such as ERC20 transfers, where artifact sizes remain modest, the computational cost of redundant context management becomes the primary bottleneck. Modern EVMs provide highly optimized frame-level infrastructure. An effective strategy must leverage these mechanisms rather than duplicating them.

Tracing itself also introduces computational overhead. Existing systems employ different strategies to mitigate this cost. Forerunner and Seer perform tracing in the transaction pool, asynchronously profiling pending transactions before block execution. This removes tracing from the critical path, though artifact I/O costs remain during actual execution. ParallelEVM performs online tracing during parallel execution, where each thread simultaneously executes and traces transactions before resolving conflicts through selective re-execution. This strategy reduces memory I/O by avoiding large pre-generated artifacts and relying instead on runtime dependency tracking.

However, on modern EVMs like Revm, both strategies become prohibitively expensive. We instrumented Revm with a comprehensive tracer recording stack, memory, and storage dependencies during execution. Single-transaction latency increased from 60 µs to over 300 µs, representing a 5× slowdown on the critical path. This overhead stems from per-opcode hook invocations, shadow data structure maintenance, and context switches between the EVM engine and tracing infrastructure. When baseline execution operates at this speed, these mechanisms consume time comparable to the operations being traced.

Furthermore, the purported advantage of full dependency tracking proves illusory in practice. EVMTracer proposed exploiting opcode-level dependencies to parallelize execution within a single transaction. We implemented a dependency-graph-driven parallel executor for Revm that schedules independent opcodes across multiple threads based on recorded dependencies. Contrary to theoretical models, the parallel version consistently underperformed its optimized serial counterpart. The root cause is fundamental. At nanosecond execution granularity, thread synchronization and communication overhead far exceeds the time saved by parallel instruction execution. When individual opcodes complete in 20 nanoseconds but thread coordination requires hundreds of nanoseconds, parallelism merely adds overhead.

\noindent\textbf{Design Principle 1.} \emph{Minimize analysis overhead through lightweight, asynchronous tracing that leverages host EVM infrastructure.} To achieve meaningful speedups on modern EVMs, an optimization strategy must produce compact artifacts to minimize I/O costs, operate asynchronously off the critical execution path, reuse the host EVM's optimized frame management and state access mechanisms, and capture only information that enables practical, low-overhead optimizations.

This principle establishes the next constraint. Any tracing mechanism must be selective, focusing on the subset of execution state that yields the highest optimization return per byte of trace data. The question then becomes what subset of execution dependencies we should capture.

\subsection{Frame-Level Reuse over Monolithic Traces}
\label{sec:frame-level-caching}

\input{figures/pareto-cumulative}

The previous section established the need for lightweight tracing. However, reducing tracing overhead alone is insufficient. We must also maximize the value extracted from each traced execution. This requires addressing two interrelated design decisions regarding the granularity of optimization artifacts and the mechanisms for enabling their reuse across transactions.

Existing path-driven systems adopt transaction-level artifact organization, where each transaction's optimization artifacts are generated and discarded immediately after use. This granularity choice introduces severe limitations for reuse. Consider three transactions where Tx1 calls contracts C1→C2, Tx2 calls C3→C4, and Tx3 calls C1→C3. Under transaction-level organization, Tx3 cannot reuse cached artifacts from Tx1 or Tx2, despite sharing individual contract call frames. The cache key is the entire transaction's execution pattern, the specific sequence C1→C3, rather than individual frame paths. Even though the C1 invocation in Tx3 may be identical to the C1 invocation in Tx1, and the C3 invocation identical to that in Tx2, the system is forced to retrace both frames because the transaction-level composition is novel. This use-once-discard model represents significant wasted effort. Even when constituent frames execute along identical paths, the system repeats the full tracing and optimization process.

Frame-level organization resolves this limitation by treating each contract call frame as an independent optimization unit. In the Tx3 example, the individual C1 frame can be recognized and reused from Tx1's cache, and the C3 frame from Tx2's cache. Only genuinely novel frame-level execution patterns require new tracing. This compositional reuse substantially increases effective cache coverage. Rather than requiring exact transaction-level matches, the system benefits from any partial overlap in the frame-level call graph.

To validate this design choice and quantify the economic value of frame-level caching, we conducted large-scale path analysis on Ethereum mainnet blocks 19,600,000 through 19,605,000. For each contract call frame, we compute a path identifier by hashing the sequence of executed opcodes within that frame, then track the frequency distribution of these unique path identifiers across all frames in the analyzed blocks.

The results in Figure~\ref{fig:pareto-cumulative} reveal a pronounced Pareto distribution with strong path locality. The top 10\% of unique paths account for over 90\% of all frame executions. Further decomposition shows extreme concentration. The top 1\% of paths handle 70\% of executions, and the top 5\% handle 85\%. This distribution remains stable across different block ranges and time periods, indicating that path locality is a fundamental characteristic of EVM execution rather than transient behavior.

This empirical finding confirms the value of persistent, frame-level caching. For high-frequency paths in the top 1\%, a single tracing cost can be amortized across hundreds or thousands of subsequent executions. When a path appears 1000 times but requires tracing only once, the effective per-execution overhead becomes negligible, fundamentally changing the cost-benefit ratio compared to transaction-level use-once-discard approaches. The extreme concentration further suggests that even modest cache sizes can achieve high hit rates by retaining only the most frequently executed paths.

Furthermore, frame-level granularity aligns naturally with the EVM's execution model, which inherently manages resources and state at frame boundaries. Each call frame operates within an isolated context with its own memory space, storage view, and gas quota. This alignment allows direct reuse of mature frame management infrastructure from host EVM clients, including optimized memory allocation and storage access patterns. Transaction-level approaches, by contrast, must implement custom frame simulation logic to maintain correctness across artificially flattened execution sequences, adding complexity and potential correctness risks.

The combination of empirical path locality and architectural alignment motivates our second principle.

\noindent\textbf{Design Principle 2.} \emph{Enable cross-transaction reuse through frame-level, persistent caching.} Optimization artifacts should be organized at the frame granularity and indexed by per-frame path identifiers. This enables compositional reuse across transactions, amortizes optimization costs across high-frequency paths, and leverages host EVM infrastructure for frame management.

This principle establishes two additional constraints. First, artifact organization must respect frame boundaries. Second, caching must support persistent storage with efficient lookup. These constraints, combined with the lightweight tracing requirement from Principle 1, narrow the design space considerably. The remaining question is what specific execution information we should trace and optimize.

\subsection{Achieving Natural Gas-Semantic Equivalence}
\label{sec:gas-semantic-equivalence}

The previous sections established the need for lightweight, frame-level, path-driven optimization. However, any optimization strategy must ultimately satisfy a critical correctness requirement specific to blockchain execution. Exact preservation of gas semantics is essential because gas underpins miner incentives in Ethereum's economic model. Transaction senders pay fees proportional to gas consumed. Any optimization altering gas consumption disrupts this economic balance, either overcompensating miners when charging original gas for reduced work or undercompensating them when charging reduced gas. Maintaining gas-semantic equivalence, defined as exact matching of native execution's gas accounting, is thus an economic necessity for practical adoption.

The core challenge stems from the distinction between static-cost and dynamic-cost instructions as defined in \S\ref{sec:gas}. Static-cost instructions have compile-time-determinable gas costs, while dynamic-cost instructions have runtime-dependent costs that depend on state such as memory expansion or storage access history.

Any optimization that eliminates or reorders dynamic-cost instructions risks gas divergence, as demonstrated in \S\ref{sec:path-vs-contract} with loop hoisting. However, if we restrict optimization to static-cost instructions only, gas preservation becomes straightforward. We precompute the cumulative gas of eliminated operations and deduct it in bulk, while ensuring all dynamic-cost instructions execute natively with precise accounting.

Remarkably, our previous design decisions, driven by entirely orthogonal concerns, naturally converge to enable this strategy. First, lightweight tracing from Principle 1 led us to focus on operations with low tracing overhead while excluding those requiring substantial state tracking. This scope restriction carries an important secondary consequence. Operations excluded due to tracing cost are precisely those with dynamic gas pricing, such as memory expansion and storage access history, while the traced operations are precisely those with static costs. The excluded instructions also share a common characteristic. They perform side effects on persistent or expandable state, making their costs inherently runtime-dependent. In contrast, the included instructions operate solely on the evaluation stack, a fixed-size structure whose manipulation incurs predictable, compile-time-determinable costs.

Second, frame-level granularity from Principle 2 aligns optimization boundaries with the EVM's native gas accounting boundaries, as each call frame independently tracks gas consumption. Third, the path-driven paradigm from \S\ref{sec:path-vs-contract} constrains optimization to path-local transformations, avoiding cross-basic-block optimizations that would complicate gas accounting across control flow.

The convergence of these three orthogonal choices produces an elegant emergent property. Lightweight tracing naturally selects static-cost operations, frame-level organization aligns with gas boundaries, and path-driven optimization constrains transformations. Together, the optimization scope naturally excludes all dynamic-cost, side-effecting instructions, enabling gas-semantic equivalence without compensatory mechanisms. Each decision, motivated by distinct concerns of overhead reduction, reuse efficiency, and security, contributes guarantees that combine to solve the correctness problem. This is not a coincidence requiring delicate engineering but a robust consequence of constraint composition.

This convergence motivates our third principle.

\noindent\textbf{Design Principle 3.} \emph{Achieve gas-semantic equivalence as an emergent property of design constraints.} By restricting optimization to static-cost instructions through stack-only tracing, aligning artifact boundaries with gas accounting boundaries through frame-level organization, and constraining transformations through path-driven optimization, gas preservation arises naturally. The system can freely optimize within this scope without trading off performance against correctness.

\subsection{Synthesis: Deriving the Helios Architecture}
\label{sec:synthesis}

The three design principles of lightweight asynchronous tracing, frame-level persistent caching, and emergent gas-semantic equivalence map to Helios's architectural components with varying directness. The first two principles directly drive core components, while the third principle emerges as a natural consequence of their interaction.

Principle 1 manifests in three functional requirements. First, a selective tracer must capture execution dependencies with minimal overhead, generating compact artifacts approximately one order of magnitude smaller than full traces. Second, an asynchronous processing pipeline must decouple tracing from execution, processing artifacts off the critical path through background threads to eliminate online overhead while maintaining low artifact availability latency. Third, the tracing mechanism must leverage the host EVM's optimized native infrastructure for frame management and state access.

Principle 2 necessitates a persistent caching mechanism with per-frame-path indexing. The cache must support both deterministic lookup for known path identifiers and speculative querying for predicted hot paths. Persistent storage enables cross-transaction amortization. High-frequency paths in the top 1\% incur tracing costs once but benefit thousands of subsequent executions. Frame-level granularity enables compositional reuse, as demonstrated in \S\ref{sec:frame-level-caching}, substantially increasing effective cache coverage compared to transaction-level approaches.

An efficient execution engine is required to consume cached artifacts and accelerate hot path execution. This engine must translate traced execution sequences into an optimized execution model while preserving correctness. Principle 3 constrains the optimization scope to naturally exclude dynamic-cost operations, enabling a straightforward gas accounting strategy. The system precomputes cumulative static gas for optimized operations and deducts it in bulk at frame entry, while delegating all excluded dynamic-cost operations to the native mechanism. This design achieves exact gas equivalence without runtime compensation logic or gas recalculation overhead.

These components support two operational modes addressing distinct node workloads. Replay Mode targets archive nodes performing deterministic historical synchronization. Since all execution paths are known and unchanging, the cache can be pre-populated through one-time tracing of the entire blockchain history. Subsequent replays achieve complete cache coverage, amortizing single tracing costs across thousands of re-executions and enabling substantial speedups with minimal storage overhead, making historical replay economically viable for resource-constrained archive nodes.

Online Mode addresses full nodes processing unpredictable live transactions. Background tracing captures new paths without blocking execution per Principle 1. Dynamic caching maintains coverage of hot paths while evicting cold ones under memory pressure per Principle 2. When a cached path is available, the system executes it optimistically. On cache misses or validation failures, execution falls back to native interpretation, ensuring correctness for never-before-seen patterns.

Critically, both modes share identical core components and artifact formats, differing only in cache population strategy and execution strategy. Pre-fill versus on-demand caching distinguishes Replay Mode from Online Mode, as does deterministic versus speculative execution. This unified design allows a single implementation to span the full node-type spectrum. Quantitative performance evaluation, including detailed speedup measurements, cache hit rates, and storage overhead analysis for both operational modes across diverse workloads, is presented in \S\ref{sec:evaluation}.

Building on these three design principles and their instantiation across operational scenarios, the following section presents Helios's detailed architecture.