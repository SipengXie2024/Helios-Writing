% ============================================================
% 建议的修改方案：将 §6.3.2 和 §6.3.3 合并重组
% ============================================================

\subsection{Mainnet Workload Analysis}

\subsubsection{Storage Overhead and Coverage (RQ1)}
% ... 保持不变 ...

\subsubsection{End-to-End Speedup (RQ2 \& RQ3)}

\input{figures/combined-speedup}  % 新的合并图表

Figure~\ref{fig:combined_speedup} presents the block-level speedup distribution across all three deployment configurations. We summarize the key observations below.

\paragraph{Replay Mode.}
Replay mode assumes a precomputed transaction plan and represents the best-case scenario for archive nodes. It achieves a median speedup of 6.60$\times$ over Revm Native, with the 75th and 90th percentiles reaching 13.88$\times$ and 21.43$\times$, respectively. Only 5.4\% of blocks exhibit slowdown. The distribution skews toward high speedups (10--20$\times$ and $\geq$20$\times$ bins account for 37.6\% of blocks), confirming that deterministic path reuse enables substantial acceleration for historical block re-execution.

\paragraph{Online Mode (Full Cache).}
Without frequency filtering, Online mode must predict execution paths using only the current CallSig. It reaches a median speedup of 4.56$\times$, with 75th and 90th percentiles at 7.12$\times$ and 9.85$\times$. The distribution concentrates in the 3--10$\times$ range (63.0\% of blocks), reflecting the overhead of runtime prediction and occasional mispredictions. Still, 8.8\% of blocks are slower than baseline---higher than Replay mode due to cache lookup costs and fallback penalties.

\paragraph{Online Mode (Filtered, $f \geq 10$).}
Frequency-based filtering reduces storage to 50~MB but shifts the distribution leftward. The median speedup drops to 2.05$\times$, and the 1--2$\times$ bin now dominates (36.2\% of blocks). However, the 90th percentile remains high at 9.01$\times$, indicating that hot paths still benefit substantially. This trade-off suits latency-sensitive validators who prioritize storage efficiency over median-case acceleration.

\paragraph{Cross-Mode Comparison.}
The three distributions reveal a clear hierarchy: Replay $>$ Online (full) $>$ Online (filtered). The gap between Replay and Online stems from three factors: (i) Top-1 prediction covers only 58.0\% of executions under filtering; (ii) transaction-level rollback discards partially optimized work on mispredictions; and (iii) prediction overhead becomes visible when optimized paths execute in microseconds. We discuss potential extensions in \S\ref{sec:discussion}.

% ============================================================
% 对应的 figures/combined-speedup.tex
% ============================================================
% \begin{figure}[t]
%   \centering
%   \includegraphics[width=\columnwidth]{figures/combined_speedup_distribution.pdf}
%   \caption{Block-level speedup distribution over Revm Native across three deployment modes. Replay assumes known transaction plans; Online predicts paths at runtime. Filtering ($f \geq 10$) trades median speedup for 8$\times$ storage reduction.}
%   \label{fig:combined_speedup}
% \end{figure}
